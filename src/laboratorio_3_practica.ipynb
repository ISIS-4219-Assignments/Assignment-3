{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "46f172e3",
      "metadata": {
        "id": "46f172e3"
      },
      "source": [
        "![Machine Learning Lab](banner.jpg)\n",
        "\n",
        "# Laboratorio 3\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "1. **Exploración de Support Vector Machines (SVM)**: Analizar el comportamiento de diferentes kernels (linear, RBF, polynomial, sigmoid) en datos no linealmente separables.\n",
        "\n",
        "2. **Procesamiento del dataset de fraude**: Cargar y explorar el dataset de transacciones fraudulentas con tarjetas de crédito.\n",
        "\n",
        "3. **Implementación de pipelines**: Crear pipelines combinando StandardScaler y SVC para clasificación de fraudes.\n",
        "\n",
        "4. **Evaluación de modelos**: Analizar el rendimiento usando matrices de confusión y métricas de clasificación.\n",
        "\n",
        "5. **Optimización de hiperparámetros**: Aplicar Grid Search para encontrar los mejores parámetros del modelo SVM.\n",
        "\n",
        "## Descarga de datasets\n",
        "\n",
        "Antes de comenzar, descarguemos los datasets necesarios para el laboratorio. Estos datasets incluyen información sobre fraudes con tarjetas de crédito."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4a0a965687191c",
      "metadata": {
        "id": "5e4a0a965687191c"
      },
      "outputs": [],
      "source": [
        "!mkdir datasets\n",
        "!curl -L -o datasets/creditcardfraud.zip https://www.kaggle.com/api/v1/datasets/download/mlg-ulb/creditcardfraud\n",
        "!unzip datasets/creditcardfraud.zip -d datasets/creditcardfraud\n",
        "!ls datasets/creditcardfraud"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b708d6b8",
      "metadata": {
        "id": "b708d6b8"
      },
      "source": [
        "## Cargar librerias\n",
        "\n",
        "Importamos las librerías necesarias para usar Support Vector Machines (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "# Importar librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36e3576cc9c4542",
      "metadata": {
        "id": "f36e3576cc9c4542"
      },
      "source": [
        "## 1. Support Vector Machines (SVC) y la utilidad de los kernels\n",
        "\n",
        "### 1.1. Exploración datos sintéticos\n",
        "\n",
        "Empezaremos exploremos el uso de SVC sobre datos sintéticos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbf9c8ac1d7359e7",
      "metadata": {
        "id": "dbf9c8ac1d7359e7"
      },
      "outputs": [],
      "source": [
        "# Generar datos que no sean linealmente separables\n",
        "X_random, y_random = make_circles(n_samples=1000, factor=0.5, noise=0.1, random_state=42)\n",
        "\n",
        "# Visualizar los datos\n",
        "sns.scatterplot(x=X_random[:, 0], y=X_random[:, 1], hue=y_random)\n",
        "plt.title(\"Datos No Linealmente Separables\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af812e137ced25ce",
      "metadata": {
        "id": "af812e137ced25ce"
      },
      "source": [
        "Como se puede ver estos datos no son separables linealmente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d959f2cb085022e",
      "metadata": {
        "id": "d959f2cb085022e"
      },
      "outputs": [],
      "source": [
        "def plot_frontier_svc(X_, y_, kernel=\"linear\", C=1):\n",
        "    \"\"\"\n",
        "    Grafica la frontera de decisión de un clasificador de Máquina de Vectores de Soporte (SVM)\n",
        "    utilizando conjuntos de datos de entrenamiento y prueba.\n",
        "    El clasificador SVM es entrenado en la versión escalada de la matriz de características.\n",
        "    La función visualiza la frontera de decisión.\n",
        "\n",
        "    :param X_: ndarray\n",
        "        La matriz de características. Estos datos deben contener únicamente 2 características,\n",
        "        por ejemplo, ['Glucosa', 'Edad']\n",
        "\n",
        "    :param y_: ndarray\n",
        "        Los valores objetivo/etiquetas de clase correspondientes a las filas de la matriz\n",
        "        de características de entrada.\n",
        "\n",
        "    :param kernel: str, opcional\n",
        "        Especifica el tipo de kernel a utilizar en el algoritmo SVM. Por defecto es \"linear\".\n",
        "        Otras opciones válidas incluyen \"rbf\", \"poly\", \"sigmoid\", etc.\n",
        "\n",
        "    :param C: float, opcional\n",
        "        Parámetro de regularización. La fuerza de la regularización es inversamente\n",
        "        proporcional a C. Valor por defecto es 1.\n",
        "    \"\"\"\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_)\n",
        "\n",
        "    X_train_, X_test_, y_train_, y_test_ = train_test_split(X_scaled, y_, test_size=0.2, random_state=31)\n",
        "\n",
        "    svm = SVC(kernel=kernel, C=C)\n",
        "    svm.fit(X_train_, y_train_)\n",
        "\n",
        "    x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
        "    y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                         np.linspace(y_min, y_max, 100))\n",
        "\n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "\n",
        "    train_df = pd.DataFrame({\n",
        "        'x': X_train_[:, 0],\n",
        "        'y': X_train_[:, 1],\n",
        "        'class': y_train_,\n",
        "        'set': 'train'\n",
        "    })\n",
        "    test_df = pd.DataFrame({\n",
        "        'x': X_test_[:, 0],\n",
        "        'y': X_test_[:, 1],\n",
        "        'class': y_test_,\n",
        "        'set': 'test'\n",
        "    })\n",
        "    df = pd.concat([train_df, test_df])\n",
        "\n",
        "    sns.scatterplot(data=df, x='x', y='y', hue='class', style='set')\n",
        "    plt.xlabel(\"Componente Principal 1\")\n",
        "    plt.ylabel(\"Componente Principal 2\")\n",
        "    plt.title(f\"Frontera de Decisión del SVM kernel: {kernel} C:{C}\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cba72a20ec941b",
      "metadata": {
        "id": "1cba72a20ec941b"
      },
      "source": [
        "Veamos entonces como afecta la frontera de desicion usar distintos kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adf9e62dade59fd7",
      "metadata": {
        "id": "adf9e62dade59fd7"
      },
      "outputs": [],
      "source": [
        "plot_frontier_svc(X_random, y_random, kernel=\"linear\", C=1)\n",
        "plot_frontier_svc(X_random, y_random, kernel=\"sigmoid\", C=1)\n",
        "plot_frontier_svc(X_random, y_random, kernel=\"poly\", C=1)\n",
        "plot_frontier_svc(X_random, y_random, kernel=\"rbf\", C=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c5c8cd0d141b86",
      "metadata": {
        "id": "83c5c8cd0d141b86"
      },
      "source": [
        "## 2.0. Exploración dataset\n",
        "\n",
        "### Descripción del Dataset de Fraude en Tarjetas de Crédito\n",
        "\n",
        "El dataset que utilizaremos contiene transacciones realizadas con tarjetas de crédito durante septiembre de 2013 por portadores de tarjetas europeos. Este conjunto de datos presenta transacciones que ocurrieron en dos días, donde tenemos 492 casos de fraude de un total de 284,807 transacciones.\n",
        "\n",
        "**Características principales del dataset:**\n",
        "\n",
        "- **Variables numéricas**: Contiene únicamente variables de entrada numéricas que son el resultado de una transformación PCA (Análisis de Componentes Principales). Debido a cuestiones de confidencialidad, no se pueden proporcionar las características originales ni más información contextual sobre los datos.\n",
        "\n",
        "- **Características transformadas**: Las características V1, V2, ..., V28 son los componentes principales obtenidos con PCA.\n",
        "\n",
        "- **Características no transformadas**:\n",
        "  - **'Time'**: Contiene los segundos transcurridos entre cada transacción y la primera transacción en el dataset.\n",
        "  - **'Amount'**: Es el monto de la transacción.\n",
        "\n",
        "- **Variable objetivo**: La característica **'Class'** es la variable de respuesta que toma el valor 1 en caso de fraude y 0 en caso contrario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f6a87d335599511",
      "metadata": {
        "id": "2f6a87d335599511"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "fraud_data = pd.read_csv(\"datasets/creditcardfraud/creditcard.csv\")\n",
        "\n",
        "# Explorar el dataset\n",
        "print(fraud_data.head())\n",
        "print(fraud_data.info())\n",
        "print(fraud_data['Class'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f112e391c7461a0",
      "metadata": {
        "id": "9f112e391c7461a0"
      },
      "source": [
        "### 2.1. Separación de los datos\n",
        "\n",
        "Separamos los datos en sets de entrenamiento y testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7698071d8be13116",
      "metadata": {
        "id": "7698071d8be13116"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Separar características (X) y etiquetas (y)\n",
        "X = fraud_data.drop('Class', axis=1)\n",
        "y = fraud_data['Class']\n",
        "\n",
        "# Dividir el dataset en entrenamiento y prueba usando 30% para prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6f2a7ee10b7adaf",
      "metadata": {
        "id": "d6f2a7ee10b7adaf"
      },
      "source": [
        "### 2.2. Crear un pipeline con StandardScaler y SVC\n",
        "\n",
        "StandardScaler normaliza los datos, asegurando que todas las características tengan la misma escala , lo que es crucial para SVC, ya que es sensible a las diferencias de magnitud entre características. Tener en cuenta que la proxima celda puede tardarse aproximadamente 10 minutos en completarse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9cab64b9d99c584",
      "metadata": {
        "id": "a9cab64b9d99c584"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svc', SVC(kernel='linear', probability=True))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Reporte de Clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nPrecisión del modelo:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d84fea2932991e19",
      "metadata": {
        "id": "d84fea2932991e19"
      },
      "source": [
        "### 2.3. Matriz de confusión\n",
        "\n",
        "La matriz de confusión nos permite visualizar de forma clara el rendimiento del clasificador, mostrando la relación entre las predicciones y los valores reales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b36215259c91a6e",
      "metadata": {
        "id": "6b36215259c91a6e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Fraude', 'Fraude'],\n",
        "            yticklabels=['No Fraude', 'Fraude'])\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Real')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af57d5cd420907fc",
      "metadata": {
        "id": "af57d5cd420907fc"
      },
      "source": [
        "### 2.4. Optimización de hiperparámetros\n",
        "\n",
        "De acuerdo a lo mostrado al comienzo del laboratorio veamos si realizar cambios al kernel o la normalización puede mejorar el modelo. Debido al tamaño del dataset solo haremos GridSearch sobre una fracción de estos. Esta celda puede demorar en ejecutarse media hora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a4d7826429bfab6",
      "metadata": {
        "id": "4a4d7826429bfab6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'svc__C': [0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svc', SVC(kernel='linear', probability=True))\n",
        "])\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train.iloc[:50000], y_train.iloc[:50000])\n",
        "\n",
        "print(\"Mejores parámetros:\", grid_search.best_params_)\n",
        "print(\"Mejor puntuación:\", grid_search.best_score_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "print(\"Reporte de Clasificación (Mejor Modelo):\")\n",
        "print(classification_report(y_test, y_pred_best))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25aeb646",
      "metadata": {
        "id": "25aeb646"
      },
      "source": [
        "Aplicamos el modelo optimizado con los mejores hiperparámetros encontrados por Grid Search en todo el conjunto de prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7bd6ce0",
      "metadata": {
        "id": "e7bd6ce0"
      },
      "source": [
        "Visualizamos los resultados finales del modelo optimizado mediante una nueva matriz de confusión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a42f5398fafedbd",
      "metadata": {
        "id": "7a42f5398fafedbd"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svc', SVC(kernel='linear', C=100))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Reporte de Clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nPrecisión del modelo:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93575bdd96e02f08",
      "metadata": {
        "id": "93575bdd96e02f08"
      },
      "outputs": [],
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Fraude', 'Fraude'],\n",
        "            yticklabels=['No Fraude', 'Fraude'])\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Real')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env (3.11.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
